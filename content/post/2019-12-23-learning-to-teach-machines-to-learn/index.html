---
title: Learning to Teach Machines to Learn
author: Alison Hill
date: '2019-12-23'
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2019-12-23T08:51:41-08:00'
featured: no
disable_jquery: no
image:
  caption: 'Image by [Daniel Cheung](https://unsplash.com/photos/sCdm5DiJb8w)'
  focal_point: ''
  preview_only: no
projects: []
---



<p>I‚Äôm excited to be teaching a new workshop at the upcoming <a href="https://rstudio.com/conference/">rstudio::conf</a> in January called <em>‚ÄúIntroduction to Machine Learning with the Tidyverse‚Äù</em>, with my colleague Garrett Grolemund. Our workshop just sold out over the weekend! üéâ</p>
<p>It is always hard to develop an entirely new workshop, especially if you are doing it at the same time as learning how to use a new API. It is even harder when that API is under active development like the <a href="https://github.com/tidymodels"><strong>tidymodels</strong> ecosystem</a>! I‚Äôve been so lucky to be able to work with the tidymodels team at RStudio, <a href="https://twitter.com/topepos">Max Kuhn</a> and <a href="https://blog.davisvaughan.com/">Davis Vaughan</a>, to help shape how we tell the tidymodels story to ML beginners. But my favorite part of developing a new workshop like this has been studying how <em>others</em> teach machine learning. Spoiler alert: there are a lot of materials intended for learners that make things seem harder than they actually are! Below, I‚Äôm sharing my bookmarked resources, organized roughly in the order I think they are most helpful for beginners.</p>
<ol style="list-style-type: decimal">
<li><p><strong><a href="https://vas3k.com/blog/machine_learning/">Machine Learning for Everyone.</a> In simple words. With real-world examples. Yes, again.</strong> In my experience, the biggest hurdle to getting started is sifting through both the hype and the math. This is a readable illustrated introduction to key concepts that will help you start building your own mental model of this space. For example, <em>‚Äúthe only goal of machine learning is to predict results based on incoming data. That‚Äôs it.‚Äù</em> There you go! Start here.</p>
<p><a href="https://vas3k.com/blog/machine_learning/" target="_blank"><img src="https://i.vas3k.ru/7w1.jpg" width="80%" /></a></p></li>
<li><p><a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">A Visual Introduction to Machine Learning by r2d3</a>. This is a wonderful two-part series (that I wish would be extended!):</p>
<ul>
<li><a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">Part I: A Decision Tree</a></li>
<li><a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-2/">Part II: Model Tuning and the Bias-Variance Tradeoff</a> <br><br></li>
</ul></li>
<li><p><a href="https://supervised-ml-course.netlify.com/">Supervised Machine Learning course by Julia Silge</a> Taught with R and the <code>caret</code> package (the precursor to the in-development <strong>tidymodels</strong> ecosystem), this is a great next step in your machine learning journey as you‚Äôll start <em>doing ML</em> right away in your browser using an innovative course delivery platform. You‚Äôll also get to play with data that is <em>not</em> <code>iris</code>, <code>titanic</code>, or <code>AmesHousing</code>. This will be sweet relief because you‚Äôll find the rest of my recommended resources all basically build models to predict home prices in Ames, Iowa.</p>
<p><a href="https://supervised-ml-course.netlify.com/" target="_blank"><img src="https://raw.githubusercontent.com/juliasilge/supervised-ML-case-studies-course/master/static/social.png" width="50%" /></a></p></li>
<li><p><a href="https://bradleyboehmke.github.io/HOML/">Hands-on Machine Learning with R</a> by Bradley Boehmke &amp; Brandon Greenwell. Another great way to learn concepts plus code, although another one that focuses on the <code>caret</code> package (pre-tidymodels). Each chapter maps onto a new learning algorithm, and provides a code-through with real data from building to tuning. The authors also offer practical advice for each algorithm, and the ‚Äúfinal thoughts‚Äù sections at the end of each chapter will help you tie it all together.</p>
<p><a href="https://bradleyboehmke.github.io/HOML/" target="_blank"><img src="https://bradleyboehmke.github.io/HOML/images/homl-cover.jpg" width="40%" /></a></p>
<p>Don‚Äôt skip the ‚ÄúFundamentals‚Äù section, even if you feel like you‚Äôve got that down by now. The second chapter on the <a href="https://bradleyboehmke.github.io/HOML/process.html">modeling process</a> is especially good.</p>
<p><a href="https://bradleyboehmke.github.io/HOML/process.html" target="_blank"><img src="https://bradleyboehmke.github.io/HOML/images/modeling_process.png" width="80%" /></a></p></li>
<li><p><a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</a> by <a href="https://christophm.github.io/">Christoph Molnar</a>. If you only have time to read a single chapter, skip ahead to <a href="https://christophm.github.io/interpretable-ml-book/simple.html">Chapter 4: Interpretable Models</a>. I also appreciated the introduction section on <a href="https://christophm.github.io/interpretable-ml-book/terminology.html">terminology</a>. But the whole book is excellent and well-written.</p>
<p><a href="https://christophm.github.io/interpretable-ml-book/" target="_blank"><img src="https://christophm.github.io/interpretable-ml-book/images/title_page.jpg" width="40%" /></a></p></li>
<li><p><strong>Model evaluation, model selection, and algorithm selection in machine learning- a 4-part series by <a href="https://sebastianraschka.com/">Sebastian Raschka</a></strong>. I found this to be a great evidence-based, thorough overview of the <em>methods</em> for machine learning. I especially liked how he walks you step-by-step from the simplest methods like the holdout method up to nested cross-validation:</p>
<ul>
<li><a href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html">Part I: The Basics</a></li>
<li><a href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html">Part II: Bootstrapping &amp; uncertainties</a></li>
<li><a href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html">Part III: Cross-validation and hyperparameter tuning</a></li>
<li><a href="https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html">Part IV: Comparing the performance of machine learning models and algorithms using statistical tests and nested cross-validation</a></li>
</ul>
<p><img src="https://sebastianraschka.com/images/blog/2016/model-evaluation-selection-part1/iris-dist.png" width="50%" /></p></li>
<li><p>At this point, if you can read through the above resources and you are no longer feeling awash in new terminology, I think your vocabulary and mental model are in pretty good shape! That means you are ready for the next step, which is to read Max Kuhn and Kjell Johnson‚Äôs new book <a href="http://www.feat.engineering/">Feature Engineering and Selection: A Practical Approach for Predictive Models</a></p>
<p><a href="http://www.feat.engineering/" target="_blank"><img src="https://images.tandf.co.uk/common/jackets/amazon/978113807/9781138079229.jpg" width="40%" /></a></p>
<p>In my experience, the later chapters in this book filled in a lot of lingering questions I had about certain methods, like whether to use <a href="http://www.feat.engineering/categorical-trees.html">factor or dummy variables in tree-based models</a>. But also don‚Äôt miss the section on <a href="http://www.feat.engineering/important-concepts.html">‚Äúimportant concepts‚Äù</a> at the beginning- this <em>should</em> feel like a nice review if you‚Äôve gotten this far!</p></li>
<li><p><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">Elements of Statistical Learning</a>. The entire PDF of the book is available online. A great resource for those with a strong statistics background, and for those looking for more math and formulas.</p></li>
</ol>
<div id="other-note-worthy-resources" class="section level2">
<h2>Other note-worthy resources</h2>
<ul>
<li><p>For the highly visual learner, you may want to cue up some YouTube videos from Udacity‚Äôs <a href="https://www.youtube.com/playlist?list=PLAwxTw4SYaPnIRwl6rad_mYwEk4Gmj7Mx">‚ÄúMachine Learning for Trading‚Äù</a> course. I found these illustrations especially helpful:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=yBqf9XCpz8o&amp;list=PLAwxTw4SYaPnIRwl6rad_mYwEk4Gmj7Mx&amp;index=187&amp;t=0s">Cross-validation</a></li>
<li><a href="https://www.youtube.com/watch?v=mfzHchd5La8&amp;list=PLAwxTw4SYaPnIRwl6rad_mYwEk4Gmj7Mx&amp;index=190&amp;t=0s">Overfitting</a></li>
<li><a href="https://www.youtube.com/watch?v=Un9zObFjBH0&amp;list=PLAwxTw4SYaPnIRwl6rad_mYwEk4Gmj7Mx&amp;index=192&amp;t=0s">Ensemble learners</a></li>
<li><a href="https://www.youtube.com/watch?v=2Mg8QD0F1dQ&amp;list=PLAwxTw4SYaPnIRwl6rad_mYwEk4Gmj7Mx&amp;index=193&amp;t=0s">Bootstrap aggregating (bagging)</a></li>
<li><a href="https://www.youtube.com/watch?v=GM3CDQfQ4sw&amp;list=PLAwxTw4SYaPnIRwl6rad_mYwEk4Gmj7Mx&amp;index=195&amp;t=0s">Boosting</a></li>
</ul>
<p><img src="https://i.ytimg.com/vi/Un9zObFjBH0/maxresdefault.jpg" /><!-- --></p></li>
<li><p>Chris Albon‚Äôs Machine Learning Flashcards ($12)</p>
<p><a href="https://store.chrisalbon.com/machine-learning-flashcards"><img src="https://d31ezp3r8jwmks.cloudfront.net/variants/BhXYeMJycv8QHBQgJfjemVgz/d2e337a4f6900f8d0798c596eb0607a8e0c2fbddb6a7ab7afcd60009c119d4c7" /></a><!-- --></p></li>
<li><p><a href="https://www.shirin-glander.de/">Shirin Elsinghorst‚Äôs blog</a> (<strong>free! and so good</strong>).</p>
<p><a href="https://www.shirin-glander.de/" target="_blank"><img src="shirin.png" width="1440" /></a></p>
<p>I love her <a href="https://shirinsplayground.netlify.com/categories/sketchnotes/">sketchnotes</a>.</p>
<p><a href="https://www.shirin-glander.de/"><img src="https://shiring.github.io/netlify_images/lime_sketchnotes_guq6u5.jpg" /></a><!-- --></p></li>
<li><p>I also found Rafael Irizarry‚Äôs <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html">‚ÄúIntroduction to Machine Learning‚Äù</a>, a chapter from his <a href="https://rafalab.github.io/dsbook/">Introduction to Data Science book</a>, to have some helpful discussion.</p></li>
<li><p><a href="https://www.nature.com/articles/nmeth.4526">Machine Learning: A primer</a> by Danilo Bzdok, Martin Krzywinski &amp; Naomi Altman, from the <a href="https://www.nature.com/collections/qghhqm/pointsofsignificance">Nature Methods Points of Significance collection</a>- this collection in general is always straight-forward with great visuals. Start with the primer, then skim these:</p>
<ul>
<li><a href="https://www.nature.com/articles/nmeth.4642">Statistics versus machine learning</a></li>
<li><a href="https://www.nature.com/articles/nmeth.4551">Machine learning: supervised methods</a></li>
<li><a href="https://www.nature.com/articles/nmeth.4370">Classification and regression trees</a> (decision trees are the ‚Äúbase learner‚Äù for many ensemble methods - this is a good intro)</li>
<li><a href="https://www.nature.com/articles/nmeth.4438">Ensemble methods: bagging and random forests</a></li>
</ul>
<p><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fnmeth.4438/MediaObjects/41592_2017_Article_BFnmeth4438_Fig1_HTML.jpg?as=webp" /><!-- --></p></li>
</ul>
<p>That‚Äôs all for now- if you are taking my workshop in January I look forward to meeting you in person! If not, rest assured that all code and materials will be shared openly after the workshop. Until then, happy learning ü§ñ</p>
</div>
